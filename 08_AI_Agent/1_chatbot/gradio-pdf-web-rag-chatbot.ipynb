{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wr9ik55TAtDB",
    "outputId": "6e3cd7fb-39ca-4755-f14b-157eb7ccd9a7"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community langchain_openai pypdf faiss-cpu gradio langchain-core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cM8Fd1ZApIC"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = userdata.get('openai-api')\n",
    "TAVILY_API_KEY = userdata.get('travily-api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain gradio openai tavily-python pypdf faiss-cpu\n",
    "# coding QA Expert Chatbot using langchain and gradio as web UI. use PDF RAG with faiss vector DB to save, retrieve the chunk documents from the PDF. if run this chatbot, read the PDF files from ./files folder, splite them into chunks, save them to faiss as vector database. after that, create LLM using openai and create langchain prompt template, tools with web search using Tavily. create agents with them including the previous dialog memory. this UI using gradio is simliar to ChatBot.\n",
    "import os, re\n",
    "import glob\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 설정\n",
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_DIR = Path.cwd()\n",
    "VECTOR_DB_PATH = str(SCRIPT_DIR / 'faiss_index')\n",
    "FILES_DIRECTORY = str(SCRIPT_DIR / 'files')\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 300\n",
    "\n",
    "# OpenAI 설정 - ChatOpenAI 사용\n",
    "llm_model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PDF 파일 로드 및 벡터화\n",
    "def load_and_split_pdfs(files_directory):\n",
    "\tpdf_files = glob.glob(os.path.join(files_directory, '*.pdf'))\n",
    "\tdocuments = []\n",
    "\tfor file in pdf_files:\n",
    "\t\tloader = PyPDFLoader(file)\n",
    "\t\tdocuments.extend(loader.load())\n",
    "\tsplitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\tsplit_documents = splitter.split_documents(documents)\n",
    "\tfor i, doc in enumerate(split_documents):\n",
    "\t\tprint(f\"Document {i}: {doc.page_content[:100]}...\")  # Print the first 100 characters of each split document\n",
    "\treturn splitter.split_documents(documents)\n",
    "\n",
    "# 3. FAISS 벡터DB 저장\n",
    "def save_to_faiss(documents):\n",
    "\tvectordb = FAISS.from_documents(documents, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY))\n",
    "\tvectordb.save_local(VECTOR_DB_PATH)\n",
    "\tprint(f\"FAISS vector database saved to {VECTOR_DB_PATH}\")\n",
    "\treturn vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RAG Retrieval QA 체인 생성\n",
    "def create_retrieval_qa(vectordb):\n",
    "\tretriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={'k': 3, 'lambda_mult': 0.25})\n",
    "\treturn retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCommon agent Types:\\n\\nzero-shot-react-description:\\n\\tUses the ReAct (Reasoning + Acting) framework.\\n\\tSelects tools and generates responses based on tool descriptions.\\n\\tBest for scenarios where the agent needs to reason and act without prior context.\\n\\nchat-zero-shot-react-description:\\n\\tSimilar to zero-shot-react-description, but optimized for chat-based interactions.\\n\\tUseful for conversational agents.\\n\\nchat-conversational-react-description:\\n\\tDesigned for conversational agents with memory.\\n\\tKeeps track of the conversation history to provide context-aware responses.\\n\\tThis is the agent type used in your code.\\n\\nself-ask-with-search:\\n\\tDesigned for agents that need to ask clarifying questions before answering.\\n\\tOften used with search tools.\\n\\nreact-docstore:\\n\\tOptimized for retrieving and reasoning over documents in a docstore.\\n\\tUseful for document-based question answering.\\n\\nconversational-react-description:\\n\\tSimilar to chat-conversational-react-description, but without explicit chat optimizations.\\n\\tIncludes memory for context-aware responses.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Common agent Types:\n",
    "\n",
    "zero-shot-react-description:\n",
    "\tUses the ReAct (Reasoning + Acting) framework.\n",
    "\tSelects tools and generates responses based on tool descriptions.\n",
    "\tBest for scenarios where the agent needs to reason and act without prior context.\n",
    "\n",
    "chat-zero-shot-react-description:\n",
    "\tSimilar to zero-shot-react-description, but optimized for chat-based interactions.\n",
    "\tUseful for conversational agents.\n",
    "\n",
    "chat-conversational-react-description:\n",
    "\tDesigned for conversational agents with memory.\n",
    "\tKeeps track of the conversation history to provide context-aware responses.\n",
    "\tThis is the agent type used in your code.\n",
    "\n",
    "self-ask-with-search:\n",
    "\tDesigned for agents that need to ask clarifying questions before answering.\n",
    "\tOften used with search tools.\n",
    "\n",
    "react-docstore:\n",
    "\tOptimized for retrieving and reasoning over documents in a docstore.\n",
    "\tUseful for document-based question answering.\n",
    "\n",
    "conversational-react-description:\n",
    "\tSimilar to chat-conversational-react-description, but without explicit chat optimizations.\n",
    "\tIncludes memory for context-aware responses.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Query with tools\n",
    "conversation_history = []\n",
    "\n",
    "def query_with_tools(user_input):\n",
    "\t# Try PDF QA first\n",
    "\ttry:\n",
    "\t\tdocs = qa_chain.invoke(user_input)\n",
    "\t\tcontext = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\t\tprompt = f\"Based on the following context, answer the question:\\n\\nContext: {context}\\n\\nQuestion: {user_input}\\n\\nAnswer:\"\n",
    "\t\tresult = llm_model.invoke([HumanMessage(content=prompt)])\n",
    "\t\treturn result.content\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"PDF QA error: {e}\")\n",
    "\t\n",
    "\t# Fallback to web search\n",
    "\ttry:\n",
    "\t\tsearch_tool = TavilySearchResults(max_results=5, tavily_api_key=TAVILY_API_KEY)\n",
    "\t\tsearch_results = search_tool.invoke(user_input)\n",
    "\t\treturn str(search_results)\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Web search error: {e}\")\n",
    "\t\n",
    "\t# Direct LLM response\n",
    "\tresult = llm_model.invoke([HumanMessage(content=user_input)])\n",
    "\treturn result.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action_input(text):\n",
    "\t# \"action_input\": \"...\" 패턴을 정규식으로 추출\n",
    "\tpattern = r'\"action_input\"\\s*:\\s*\"([^\"]+)\"'\n",
    "\tmatch = re.search(pattern, text, re.DOTALL)\n",
    "\tif match:\n",
    "\t\treturn match.group(1)\n",
    "\treturn None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://7c8beb1626411656ad.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7c8beb1626411656ad.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\venv_lmm\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\venv_lmm\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\venv_lmm\\Lib\\site-packages\\gradio\\blocks.py\", line 2117, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\venv_lmm\\Lib\\site-packages\\gradio\\blocks.py\", line 1894, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\venv_lmm\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 690, in postprocess\n",
      "    self._check_format(value)\n",
      "  File \"c:\\ProgramData\\miniconda3\\envs\\venv_lmm\\Lib\\site-packages\\gradio\\components\\chatbot.py\", line 400, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n"
     ]
    }
   ],
   "source": [
    "# 초기화 과정\n",
    "if not os.path.exists(VECTOR_DB_PATH):\n",
    "    os.makedirs(VECTOR_DB_PATH, exist_ok=True)\n",
    "    docs = load_and_split_pdfs(FILES_DIRECTORY)\n",
    "    save_to_faiss(docs)\n",
    "\n",
    "vectordb = FAISS.load_local(VECTOR_DB_PATH, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), allow_dangerous_deserialization=True)\n",
    "qa_chain = create_retrieval_qa(vectordb)\n",
    "\n",
    "def chatbot_interface(user_input, history):\n",
    "    try:\n",
    "        response = query_with_tools(user_input)\n",
    "        history = history + [(user_input, response)]\n",
    "    except Exception as e:\n",
    "        msg = f\"Error: {str(e)}\"\n",
    "        print(msg)\n",
    "        response = extract_action_input(str(e))\n",
    "        if response == None:\n",
    "            response = str(e)\n",
    "        history = history + [(user_input, response)]\n",
    "\n",
    "    return history, history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Coding QA Expert Chatbot (PDF + Web Search)\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"질문을 입력하세요...\")\n",
    "\n",
    "    clear = gr.Button(\"초기화\")\n",
    "\n",
    "    state = gr.State([])\n",
    "    msg.submit(chatbot_interface, [msg, state], [chatbot, state])\n",
    "    clear.click(lambda: ([], []), None, [chatbot, state])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
