{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SHsG8CojnWi5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens(shape=torch.Size([1, 9])): tensor([[ 101, 2023, 2003, 2019, 7861, 8270, 4667, 1012,  102]])\n",
      "shape of word embeddings: torch.Size([1, 9, 768])\n",
      "word=[CLS]     , token=101\t embedding=tensor([ 0.0390, -0.0123, -0.0208, -0.0005], grad_fn=<SliceBackward0>)\n",
      "word=this      , token=2023\t embedding=tensor([-0.0558,  0.0151,  0.0031,  0.0055], grad_fn=<SliceBackward0>)\n",
      "word=is        , token=2003\t embedding=tensor([-0.0440, -0.0236, -0.0283, -0.0123], grad_fn=<SliceBackward0>)\n",
      "word=an        , token=2019\t embedding=tensor([-0.0209, -0.0007, -0.0224,  0.0082], grad_fn=<SliceBackward0>)\n",
      "word=em        , token=7861\t embedding=tensor([-0.0563, -0.0930, -0.0878, -0.0234], grad_fn=<SliceBackward0>)\n",
      "word=##bed     , token=8270\t embedding=tensor([ 0.0058, -0.0398,  0.0256, -0.0930], grad_fn=<SliceBackward0>)\n",
      "word=##ding    , token=4667\t embedding=tensor([-0.0580, -0.1073,  0.0210, -0.0589], grad_fn=<SliceBackward0>)\n",
      "word=.         , token=1012\t embedding=tensor([-0.0244, -0.0138, -0.0078, -0.0222], grad_fn=<SliceBackward0>)\n",
      "word=[SEP]     , token=102\t embedding=tensor([-0.0199, -0.0095, -0.0099, -0.0119], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel  # BERT 토크나이저와 모델 불러오기\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")  # 사전 훈련된 토크나이저 로드\n",
    "tokens = tokenizer.encode('This is an embedding.', return_tensors='pt')  # 텍스트를 토큰 ID로 변환\n",
    "print(f\"Tokens(shape={tokens.shape}): {tokens}\")  # 토큰 shape과 ID 출력\n",
    "\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")  # 사전 훈련된 BERT 모델 로드\n",
    "word_embeddings = model.embeddings.word_embeddings(tokens)  # 토큰 ID를 임베딩 벡터로 변환\n",
    "print('shape of word embeddings:', word_embeddings.shape)  # 임베딩 차원 정보 출력 (토큰수 x 768차원)\n",
    "for i, e in enumerate(word_embeddings[0]):  # 각 토큰의 임베딩 벡터를 순회하며\n",
    "\tword = tokenizer.decode([tokens[0][i]])\n",
    "\ttoken = tokens[0][i]\n",
    "\tprint(f\"word={word:10s}, token={token}\\t embedding={e[:4]}\")  # 768차원 중 처음 4차원만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens(shape=torch.Size([1, 7])): tensor([[  101,  2023,  2003,  2019, 30522,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_tokens(['embedding'])\n",
    "tokens = tokenizer.encode('This is an embedding.', return_tensors='pt')  # 텍스트를 토큰 ID로 변환\n",
    "print(f\"Tokens(shape={tokens.shape}): {tokens}\")  # 토큰 shape과 ID 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO/v0gsKmJJ0gjxD4Uw22nd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
