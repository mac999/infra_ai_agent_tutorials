{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2928,
     "status": "ok",
     "timestamp": 1745736544685,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "Cwwrpol9_LvH",
    "outputId": "7a42366b-3a16-4ae4-c3b0-d4ab203780ca"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community langchain_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10695,
     "status": "ok",
     "timestamp": 1745736507214,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "QGXYLlM9_K8D"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "key = userdata.get('openai-api')\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2965,
     "status": "ok",
     "timestamp": 1745736550398,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "F48u_Ro1_Lxf"
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1745736553809,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "yhFaYH3h_zjo"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", openai_api_key=key)\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens\\n')\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16108,
     "status": "ok",
     "timestamp": 1745736570843,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "0hymPdjT_z2X",
    "outputId": "3f566542-9ecd-460a-87e3-dc90f5a7eedc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-0a793f9e4826>:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  conversation = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
      "<ipython-input-9-0a793f9e4826>:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
      "<ipython-input-8-c971ff2cf5a7>:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1173 tokens\n",
      "\n",
      "That's excellent! Large Language Models like GPT-3 are already quite powerful in generating human-like text based on the data they were trained on. However, their performance can be significantly enhanced with the integration of external knowledge bases.\n",
      "\n",
      "There are a few ways to achieve this. One method is by creating a hybrid model that leverages large transformer models and information retrieval systems. For instance, a chatbot could use GPT-3 to generate queries, fetch relevant documents from a database using an information retrieval system, like Elasticsearch, and then use the same language model to generate a response from the retrieved documents. All these steps can be performed in near real-time and the quality of responses becomes much better as the external database can be periodically updated with new information.\n",
      "\n",
      "Another approach can be to fine-tune the large language models with domain-specific data. This allows them to provide more accurate and context-specific answers. An example of this would be training a ChatGPT model with medical textbooks and guidelines to build a chatbot specifically for health-related questions. But in this method, the inferencing will be limited to the data in the fine-tuning dataset and won't update by itself in the future.\n",
      "\n",
      "But there might be challenges with these methods too, like how to integrate the knowledge base without reducing the generative capability of the model, how to fact-check the dynamism of external data, and questions concerning context-awareness.\n",
      "\n",
      "One crucial thing to remember is that AI doesn't 'understand' the information in the same way humans do, it processes based on patterns and therefore can't independently verify the accuracy of the information. Thus, the quality of the external knowledge base is of utmost importance.\n",
      "\n",
      "Currently, research is being conducted to overcome these challenges and to build more effective and intelligent systems. Do you have a specific domain or use-case in mind for integrating external knowledge with large language models?\n"
     ]
    }
   ],
   "source": [
    "# Create a conversation chain with memory\n",
    "conversation = ConversationChain(llm=llm, memory=ConversationSummaryMemory(llm=llm))\n",
    "\n",
    "# Example usage\n",
    "output = count_tokens(conversation, \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpfxO0kLAdg4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNf/tSF4kP3enVdQju4fWrR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
