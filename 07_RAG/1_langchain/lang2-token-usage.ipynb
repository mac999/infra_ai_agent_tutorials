{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2928,
     "status": "ok",
     "timestamp": 1745736544685,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "Cwwrpol9_LvH",
    "outputId": "7a42366b-3a16-4ae4-c3b0-d4ab203780ca"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community langchain_openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10695,
     "status": "ok",
     "timestamp": 1745736507214,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "QGXYLlM9_K8D"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "key = userdata.get('openai-api')\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2965,
     "status": "ok",
     "timestamp": 1745736550398,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "F48u_Ro1_Lxf"
   },
   "outputs": [],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1745736553809,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "yhFaYH3h_zjo"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", openai_api_key=key)\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def count_tokens(query):\n",
    "    with get_openai_callback() as cb:\n",
    "        # Add user message to history\n",
    "        conversation_history.append(HumanMessage(content=query))\n",
    "        \n",
    "        # Get response\n",
    "        result = llm.invoke(conversation_history)\n",
    "        \n",
    "        # Add AI response to history\n",
    "        conversation_history.append(AIMessage(content=result.content))\n",
    "        \n",
    "        print(f'Spent a total of {cb.total_tokens} tokens\\n')\n",
    "    return result.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16108,
     "status": "ok",
     "timestamp": 1745736570843,
     "user": {
      "displayName": "Taewook Kang",
      "userId": "11578752810222612195"
     },
     "user_tz": -540
    },
    "id": "0hymPdjT_z2X",
    "outputId": "3f566542-9ecd-460a-87e3-dc90f5a7eedc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 371 tokens\n",
      "\n",
      "Integrating a large language model like OpenAI's GPT-3 with external knowledge can significantly improve its capability to generate more precise, relevant, and creative responses. \n",
      "\n",
      "For instance, an external knowledge source like a database or an API could enhance its capacity to deliver up-to-date and accurate data. As of now, the language model only relies on a pre-encoded static knowledge from its last training period. This model doesn't have the ability to update its knowledge post-training. \n",
      "\n",
      "Integration with external knowledge sources could address this challenge in several ways. It could allow the model to pull in recent information, correcting its misjudgments and enabling real-time interaction with a more dynamic source of knowledge.\n",
      "\n",
      "One way could be a combined system where the model works together with an external database, like an SQL system, drafting inquiries based on the user input and retrieving accurate responses. It might also check the validity of a piece of information against a trusted reference database.\n",
      "\n",
      "However, integrating external knowledge sources also brings about challenges. These include maintaining coherency during the conversation, dealing with the external systems' latency, or managing an additional access to a potentially sensitive data source that comes with security risks.\n",
      "\n",
      "Defining the technical integration between such a language model and an external knowledge source, ensuring the compatibility between the two systems, and achieving an efficient real-time communication can be difficult.\n",
      "\n",
      "Furthermore, ensuring that the model appropriately interprets and uses the data received from an external source is another challenge. This is because the model is pre-trained and lacks the ability to learn from the new data it receives post-training.\n",
      "\n",
      "Despite these challenges, the integration of large language models with external knowledge sources is a promising approach that could significantly improved user interaction and open up new use-cases.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "output = count_tokens(\"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpfxO0kLAdg4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNf/tSF4kP3enVdQju4fWrR",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
