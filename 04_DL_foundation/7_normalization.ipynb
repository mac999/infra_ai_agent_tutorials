{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4b8e9d",
   "metadata": {},
   "source": [
    "## Normalizatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c1ccd",
   "metadata": {},
   "source": [
    "#### L1 정규화 (Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629a7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Loss: 1.4051369428634644\n",
      "L1 Regularized Total Loss: 1.4070558547973633\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "l1_lambda = 0.001 # L1 정규화 강도\n",
    "\n",
    "model = nn.Linear(in_features=10, out_features=1) # 간단한 모델 정의\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "inputs = torch.randn(32, 10)  # 임의 입력데이터\n",
    "labels = torch.randn(32, 1)   # 임의 타겟데이터\n",
    "\n",
    "optimizer.zero_grad()  # 1 단계 학습\n",
    "outputs = model(inputs)\n",
    "base_loss = criterion(outputs, labels) # 기본 손실\n",
    "\n",
    "l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "total_loss = base_loss + l1_lambda * l1_norm # L1 페널티가 추가된 최종 손실\n",
    "\n",
    "total_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Base Loss: {base_loss.item()}\")\n",
    "print(f\"L1 Regularized Total Loss: {total_loss.item()}\") # L1 항 때문에 기본 손실보다 큼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5fa67",
   "metadata": {},
   "source": [
    "#### L2 정규화 (Weight Decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84e5cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Regularized Loss: 1.47163724899292\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "model = nn.Linear(in_features=10, out_features=1) # 간단한 모델 정의\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, \n",
    "                       weight_decay=1e-5) # 1e-5가 L2 정규화 강도\n",
    "\n",
    "inputs = torch.randn(32, 10)\n",
    "labels = torch.randn(32, 1)\n",
    "\n",
    "optimizer.zero_grad()  # 1 단계 학습 (L1과 달리 손실 함수 수정 불필요)\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"L2 Regularized Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982db1aa",
   "metadata": {},
   "source": [
    "#### 드롭아웃 (Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88d27e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4145, 0.2331, 0.0000,  ..., 0.0000, 0.0000, 0.4686],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2751,  ..., 0.0000, 0.7619, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.6044, 0.0000,  ..., 0.0000, 0.0000, 0.0046],\n",
      "        [0.9105, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.7438],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),  # p=0.5는 훈련 중 50%의 뉴런을 비활성화한다는 의미\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "inputs = torch.randn(32, 784)\n",
    "\n",
    "model.train()\n",
    "train_output = model(inputs)\n",
    "\n",
    "with torch.no_grad(): # model의 첫 Linear, ReLU, Dropout까지 실행\n",
    "\tx = model[0](inputs)\n",
    "\tx = model[1](x)\n",
    "\tdropout_output = model[2](x)\n",
    "print(dropout_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a019edb",
   "metadata": {},
   "source": [
    "#### 배치 정규화 (Batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf2703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before BatchNorm ---\n",
      "Mean: -0.0003\n",
      "Std:  0.5889\n",
      "\n",
      "--- After BatchNorm ---\n",
      "Mean: -0.0000\n",
      "Std:  1.0001\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "model_mlp = nn.Sequential(\n",
    "    nn.Linear(20, 128),\n",
    "    nn.BatchNorm1d(128) # Linear 출력 128개를 정규화\n",
    ")\n",
    "model_mlp.train() # BatchNorm은 train 모드에서 실행해야 함\n",
    "\n",
    "inputs = torch.randn(32, 20) \n",
    "\n",
    "linear_output = model_mlp[0](inputs) \n",
    "print(\"Before BatchNorm\")\n",
    "print(f\"Mean: {linear_output.mean().item():.4f}\") # 128개 특성(채널) 전체의 평균/표준편차\n",
    "print(f\"Std:  {linear_output.std().item():.4f}\")\n",
    "\n",
    "bn_output = model_mlp(inputs) # BatchNorm까지 통과\n",
    "print(\"\\nAfter BatchNorm\") # 128개 특성 전체의 평균은 0에, 표준편차는 1에 가깝게 보정됨\n",
    "print(f\"Mean: {bn_output.mean().item():.4f}\") # 0에 매우 가까움\n",
    "print(f\"Std:  {bn_output.std().item():.4f}\")  # 1에 매우 가까움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284f9ce",
   "metadata": {},
   "source": [
    "#### 레이어 정규화 (Layer Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before LayerNorm (Sample 0) ---\n",
      "Mean: 2.7334\n",
      "Std:  1.3637\n",
      "\n",
      "--- After LayerNorm (Sample 0) ---\n",
      "Mean: 0.0000\n",
      "Std:  1.0127\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "# (N, SeqLen, Features) 형태의 3D 텐서를 가정\n",
    "batch_size = 4\n",
    "seq_len = 5\n",
    "embedding_dim = 8 # 정규화할 특성의 차원\n",
    "\n",
    "input_tensor = torch.rand(batch_size, seq_len, embedding_dim) * 5 # 입력\n",
    "\n",
    "layer_norm = nn.LayerNorm(normalized_shape=embedding_dim) \n",
    "output_tensor = layer_norm(input_tensor) #  레이어 정규화 적용\n",
    "\n",
    "print(\"Before LayerNorm (Sample 0)\")\n",
    "print(f\"Mean: {input_tensor[0].mean().item():.4f}\") # 0번 샘플의 평균/표준편차\n",
    "print(f\"Std:  {input_tensor[0].std().item():.4f}\")\n",
    "\n",
    "print(\"\\nAfter LayerNorm (Sample 0)\") # 0번 샘플이 평균 0, 표준편차 1로 정규화됨\n",
    "print(f\"Mean: {output_tensor[0].mean().item():.4f}\") # 0에 매우 가까움\n",
    "print(f\"Std:  {output_tensor[0].std().item():.4f}\")  # 1에 매우 가까움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b493a",
   "metadata": {},
   "source": [
    "#### 그룹 정규화 (Group Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fe32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before GroupNorm (Sample 0, Group 0) ---\n",
      "Mean: 2.4736\n",
      "Std:  1.4974\n",
      "\n",
      "--- After GroupNorm (Sample 0, Group 0) ---\n",
      "Mean: 0.0000\n",
      "Std:  1.0102\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "\n",
    "# (N, C, H, W) 형태의 4D 텐서를 가정 (CNN)\n",
    "batch_size = 4\n",
    "num_channels = 6\n",
    "H, W = 5, 5\n",
    "\n",
    "input_tensor = torch.rand(batch_size, num_channels, H, W) * 5 # 입력\n",
    "\n",
    "num_groups = 3 # 6개 채널을 3개 그룹으로 나눔 (그룹당 2개 채널)\n",
    "group_norm = nn.GroupNorm(num_groups=num_groups, \n",
    "                          num_channels=num_channels) # GroupNorm 레이어 정의\n",
    "\n",
    "output_tensor = group_norm(input_tensor) # 그룹 정규화 적용\n",
    "\n",
    "print(\"Before GroupNorm (Sample 0, Group 0)\")\n",
    "group0_input = input_tensor[0, :2, :, :] # 0 ~ 1번 채널\n",
    "print(f\"Mean: {group0_input.mean().item():.4f}\")\n",
    "print(f\"Std:  {group0_input.std().item():.4f}\")\n",
    "\n",
    "print(\"\\nAfter GroupNorm (Sample 0, Group 0)\")\n",
    "group0_output = output_tensor[0, :2, :, :] # 0번 샘플의 0번 그룹이 평균 0, 표준편차 1로 정규화됨\n",
    "print(f\"Mean: {group0_output.mean().item():.4f}\") # 0에 매우 가까움\n",
    "print(f\"Std:  {group0_output.std().item():.4f}\")  # 1에 매우 가까움"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
