{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE (Byte Pair Encoding) 모델 훈련 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰: ['low', 'lower', 'lowest']\n",
      "토큰 ID: [15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# BPE 토크나이저 초기화\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 트레이너 설정 및 학습\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\"], vocab_size=50)\n",
    "training_data = [\"low lower lowest\", \"new newer newest\"]\n",
    "tokenizer.train_from_iterator(training_data, trainer)\n",
    "\n",
    "# 토크나이징 결과\n",
    "result = tokenizer.encode(\"low lower lowest\")\n",
    "print(f\"토큰: {result.tokens}\")\n",
    "print(f\"토큰 ID: {result.ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사전 학습된 BPE 모델 사용방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본: 'Artificial intelligence is transforming our world'\n",
      "토큰: ['Art', 'ificial', 'Ġintelligence', 'Ġis', 'Ġtransforming', 'Ġour', 'Ġworld']\n",
      "토큰 ID: [8001, 9542, 4430, 318, 25449, 674, 995]\n",
      "'AI is cool'\n",
      "  토큰: ['AI', 'Ġis', 'Ġcool']\n",
      "  ID: [20185, 318, 3608]\n",
      "\n",
      "'supercalifragilisticexpialidocious'\n",
      "  토큰: ['super', 'cal', 'if', 'rag', 'il', 'ist', 'ice', 'xp', 'ial', 'id', 'ocious']\n",
      "  ID: [16668, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
      "\n",
      "'COVID-19'\n",
      "  토큰: ['CO', 'VID', '-', '19']\n",
      "  ID: [8220, 11008, 12, 1129]\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer # 많이 사용되는 BPE 모델\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # GPT-2 BPE 토크나이저\n",
    "\n",
    "test_text = \"Artificial intelligence is transforming our world\"\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "token_ids = tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"원본: '{test_text}'\")\n",
    "print(f\"토큰: {tokens}\")\n",
    "print(f\"토큰 ID: {token_ids}\")\n",
    "\n",
    "examples = [\"AI is cool\", \"supercalifragilisticexpialidocious\", \"COVID-19\"]\n",
    "for text in examples:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    print(f\"'{text}'\")\n",
    "    print(f\"  토큰: {tokens}\")\n",
    "    print(f\"  ID: {token_ids}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE 직접 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트: low lower lowest\n",
      "초기 토큰화: {'l o w </w>': 1, 'l o w e r </w>': 1, 'l o w e s t </w>': 1}\n",
      "초기 어휘 크기: 8\n",
      "\n",
      "=== 반복 1 ===\n",
      "현재 바이그램 빈도: {('l', 'o'): 3, ('o', 'w'): 3, ('w', 'e'): 2, ('w', '</w>'): 1, ('e', 'r'): 1, ('r', '</w>'): 1, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1}\n",
      "선택된 바이그램: '('l', 'o')' (빈도: 3)\n",
      "병합 후: {'lo w </w>': 1, 'lo w e r </w>': 1, 'lo w e s t </w>': 1}\n",
      "\n",
      "=== 반복 2 ===\n",
      "현재 바이그램 빈도: {('lo', 'w'): 3, ('w', 'e'): 2, ('w', '</w>'): 1, ('e', 'r'): 1, ('r', '</w>'): 1, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1}\n",
      "선택된 바이그램: '('lo', 'w')' (빈도: 3)\n",
      "병합 후: {'low </w>': 1, 'low e r </w>': 1, 'low e s t </w>': 1}\n",
      "최종 BPE 어휘: ['</w>', 'e', 'low', 'r', 's', 't']\n",
      "어휘 크기 변화: 초기 11개 → 최종 6개\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_word_tokens(text): # 텍스트를 단어 단위로 분리하고 문자 단위로 토큰화\n",
    "\twords = text.lower().split()\n",
    "\tword_tokens = {}\n",
    "\tfor word in words:\n",
    "\t\ttokens = list(word) + ['</w>'] # 각 문자를 분리하고 단어 끝에 </w> 추가\n",
    "\t\tword_tokens[' '.join(tokens)] = word_tokens.get(' '.join(tokens), 0) + 1\n",
    "\treturn word_tokens\n",
    "\n",
    "def get_bi_grams(word_tokens): # 모든 바이그램과 빈도수 계산\n",
    "\tbi_grams = defaultdict(int)\n",
    "\tfor word, freq in word_tokens.items():\n",
    "\t\tsymbols = word.split()\n",
    "\t\tfor i in range(len(symbols) - 1):\n",
    "\t\t\tbi_grams[(symbols[i], symbols[i+1])] += freq\n",
    "\treturn bi_grams\n",
    "\n",
    "def merge_symbols(pair, word_tokens): # 선택된 바이그램을 병합\n",
    "\tnew_word_tokens = {}\n",
    "\tbi_gram = ' '.join(pair)\n",
    "\treplacement = ''.join(pair)\n",
    "\t\n",
    "\tfor word in word_tokens:\n",
    "\t\tnew_word = word.replace(bi_gram, replacement)\n",
    "\t\tnew_word_tokens[new_word] = word_tokens[word]\n",
    "\treturn new_word_tokens\n",
    "\n",
    "# BPE 예시 - 더 명확한 결과를 위한 텍스트\n",
    "text = \"low lower lowest\"\n",
    "print(f\"원본 텍스트: {text}\")\n",
    "\n",
    "# 1. 초기 토큰화\n",
    "word_tokens = get_word_tokens(text)\n",
    "print(f\"초기 토큰화: {word_tokens}\")\n",
    "print(f\"초기 어휘 크기: {len(set(' '.join(word_tokens.keys()).split()))}\")\n",
    "\n",
    "# 2. BPE 학습 과정 상세히 보기\n",
    "for i in range(2):  # 더 많은 병합 과정 관찰\n",
    "\tbi_grams = get_bi_grams(word_tokens)\n",
    "\tif not bi_grams:\n",
    "\t\tbreak\n",
    "\t\n",
    "\t# 모든 바이그램과 빈도 출력\n",
    "\tprint(f\"\\n=== 반복 {i+1} ===\")\n",
    "\tprint(f\"현재 바이그램 빈도: {dict(sorted(bi_grams.items(), key=lambda x: x[1], reverse=True))}\")\n",
    "\t\n",
    "\t# 가장 빈도가 높은 바이그램 선택\n",
    "\tbest_pair = max(bi_grams, key=bi_grams.get)\n",
    "\tprint(f\"선택된 바이그램: '{best_pair}' (빈도: {bi_grams[best_pair]})\")\n",
    "\n",
    "\t# 바이그램 병합\n",
    "\tword_tokens = merge_symbols(best_pair, word_tokens)\n",
    "\tprint(f\"병합 후: {word_tokens}\")\n",
    "\n",
    "final_vocab = set(' '.join(word_tokens.keys()).split())\n",
    "print(f\"최종 BPE 어휘: {sorted(final_vocab)}\")\n",
    "print(f\"어휘 크기 변화: 초기 {len(set('low lower lowest new newer newest'.replace(' ', '') + '</w>'))}개 → 최종 {len(final_vocab)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W8RgAAylnpo1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'united', 'states', 'and', 'russia', 'sought', 'to', 'lower', 'the', 'temperature', 'in', 'a', 'heated', 'stand', '##off', 'over', 'ukraine', ',', 'even', 'as', 'they', 'reported', 'no', 'breakthrough', '##s', 'in', 'high', '-', 'stakes', 'talks', 'on', 'friday', 'aimed', 'at', 'preventing', 'a', 'feared', 'russian', 'invasion']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "example_sen = (\n",
    "    \"\"\"\n",
    "    The United States and Russia sought to lower the temperature in a\n",
    "    heated standoff over Ukraine,even as they reported no breakthroughs\n",
    "    in high-stakes talks on Friday aimed at preventing a feared Russian invasion\n",
    "    \"\"\"\n",
    ")\n",
    "print(bert_tokenizer.tokenize(example_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNjDAzbuu7TFQjQTp33bQds",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_lmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
